import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

text = "I love deep learning"

tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1  
input_text = "I love deep"
target_word = "learning"


input_seq = tokenizer.texts_to_sequences([input_text])[0]  
target_seq = tokenizer.texts_to_sequences([target_word])[0][0]  


input_seq = np.array(input_seq).reshape(1, 3)


model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=10, input_length=3))
model.add(SimpleRNN(16))
model.add(Dense(vocab_size, activation='softmax'))
      
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(input_seq, np.array([target_seq]), epochs=500, verbose=0)

pred = model.predict(input_seq)
predicted_word_index = np.argmax(pred)
for word, index in word_index.items():
    if index == predicted_word_index:
        print("Predicted word:", word)
        break

